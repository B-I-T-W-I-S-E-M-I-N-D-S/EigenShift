# EigenShift: Eigen-based Intervention for Toxicity Reduction in LLMs 
![Status](https://img.shields.io/badge/status-active-brightgreen) 
![License](https://img.shields.io/badge/license-Creative%20Commons%20CC%204.0-blue)

> âš¡ A novel principled intervention technique for reducing toxicity in Large Language Models without compromising linguistic competence

## ğŸ“‹ Abstract

Large Language Models have demonstrated impressive fluency across diverse tasks, yet their tendency to produce toxic content remains a critical challenge for AI safety and public trust. Existing toxicity mitigation approaches primarily manipulate individual neuron activations, but these methods suffer from instability, context dependence, and often compromise the modelâ€™s core language abilities. To address these shortcomings, we investigate three key questions: the stability of neuron-level toxicity indicators, the advantages of structural (layer-wise) representations, and the interpretability of mechanisms driving toxic generation. Through extensive experiments on Jigsaw and ToxiCN datasets, we show that aggregated layer-wise features provide more robust signals than single neurons. Moreover, we observe conceptual limitations in prior works that conflate toxicity detection experts and generation experts within neuron-based interventions. To mitigate this, we propose a novel principled intervention technique, EigenShift, based on eigen-decomposition of the language modelâ€™s final output layer. This method selectively targets generation-aligned components, enabling precise toxicity suppression without impairing linguistic competence. Our method requires no additional training or fine-tuning, incurs minimal computational cost, and is grounded in rigorous theoretical analysis.

ğŸ” **Key Research Questions:**
- The stability of neuron-level toxicity indicators
- The advantages of structural (layer-wise) representations  
- The interpretability of mechanisms driving toxic generation

Through extensive experiments on Jigsaw and ToxiCN datasets, we show that aggregated layer-wise features provide more robust signals than single neurons. We propose **EigenShift**, based on eigen-decomposition of the language model's final output layer, which selectively targets generation-aligned components for precise toxicity suppression.

## ğŸ—ï¸ Architecture Overview
![Model Architecture](EigenShift-Arch.jpeg)

For more detailed methodology please go though our paper.

## âœ¨ Key Features

- ğŸ¯ **No Additional Training**: Zero fine-tuning required
- âš¡ **Minimal Computational Cost**: Efficient intervention mechanism
- ğŸ§  **Theoretically Grounded**: Based on rigorous mathematical analysis
- ğŸ›¡ï¸ **Preserves Language Abilities**: Maintains core linguistic competence
- ğŸ”§ **Easy Integration**: Simple plug-and-play solution

## ğŸ› ï¸ Installation & Setup

### Step 0: Environment Setup ğŸ

Create and activate a virtual environment using Python 3.8.10:

```bash
# Create virtual environment
python3.8 -m venv venv

# Activate environment
source venv/bin/activate  # Linux/Mac
# OR
.\venv\Scripts\activate   # Windows

# Install dependencies
pip install -r requirements.txt
```

## ğŸ“Š Pipeline Overview

```
ğŸ”„ Step 1: Generate Model Outputs
    â¬‡ï¸
ğŸ” Step 2: Extract Toxic Words  
    â¬‡ï¸
ğŸ§® Step 3: Matrix Reconstruction (Core Intervention)
    â¬‡ï¸  
ğŸ“ˆ Step 4: Evaluation After Intervention
```

---

## ğŸ® Step 1: Generate Outputs from Model

Navigate to the generation script:

```bash
cd EigenShift
python get_generations.py
```

### âš™ï¸ Configuration Required:
Before running, update these parameters in `get_generations.py`:
- `model_name` ğŸ·ï¸
- Hugging Face token ğŸ”‘
- Device (`"cuda"` or `"cpu"`) ğŸ’»

### ğŸ“ Output Location:
```
EigenShift/generations/wo-intervention/
```

### ğŸ“¦ Pre-generated Data Available:
We've included pre-generated outputs for convenience:
```
EigenShift/generations/wo-intervention/LLaMA-2-7b-hf_RTP_generations.json
```
*Contains 5000 toxic generations from LLaMA 2 7B using RealToxicPrompts (RTP)*

---

## ğŸ” Step 2: Extract Toxic Words

Uses pre-trained toxicity classifier (`s-nlp/roberta_toxicity_classifier`) to identify toxic content:

```bash
python toxic_words_extraction.py
```

### ğŸ¯ What This Does:
- Analyzes generated text for toxic content
- Extracts and categorizes toxic words
- Prepares data for intervention pipeline

### ğŸ“Š Pre-processed Output Available:
```
EigenShift/generations/wo-intervention/LLaMA-2-7b-hf_RTP_generations_roberta_toxic_words_extraction.csv
```

---

## ğŸ§® Step 3: Intervention via Matrix Reconstruction â­

**This is the core EigenShift methodology!**

```bash
python reconstruct.py
```

### ğŸ”¬ Process Breakdown:

1. **ğŸ—ï¸ Build Clusters**
   - Creates toxic/non-toxic hidden state clusters
   - Analyzes activation patterns

2. **ğŸ“ Matrix Factorization** 
   - Applies SVD (Singular Value Decomposition) on `lm_head`
   - Decomposes weight matrix into eigencomponents

3. **ğŸ“ Projection Analysis**
   - Projects hidden states onto eigenvectors
   - Maps toxicity patterns to mathematical space

4. **ğŸ“Š Delta Score Computation**
   - Computes toxicity alignment scores
   - Identifies intervention targets

5. **ğŸ›ï¸ Selective Dampening**
   - Dampens eigenvectors based on toxicity scores
   - Reconstructs optimized `lm_head` matrix

### ğŸ§  Mathematical Foundation:
The intervention leverages eigen-decomposition to:
- Target generation-aligned components specifically
- Preserve linguistic competence while reducing toxicity
- Provide interpretable intervention mechanisms

---

## ğŸ“ˆ Step 4: Evaluate After Intervention

Replace the original `lm_head` with reconstructed version and evaluate:

```bash
python evaluation_after_intervention.py
```

### ğŸ“‹ Evaluation Process:
- Loads model with reconstructed `lm_head`
- Tests on RealToxicPrompts (RTP) dataset
- Measures toxicity reduction vs. linguistic preservation
- Generates comprehensive performance metrics

---

## ğŸ“Š Baselines & Comparisons

For comparison against baseline methods, we used the official implementation from:

ğŸ”— **Reference Implementation:** [Apple ML-AURA](https://github.com/apple/ml-aura)

### ğŸ† Performance Highlights:
- Superior stability compared to neuron-level interventions
- Better context independence
- Maintained linguistic competence
- Robust across different datasets (Jigsaw, ToxiCN)


---

## ğŸ¯ Key Advantages
| Feature | Traditional Methods | EigenShift |
|---------|-------------------|------------|
| **Stability** | âŒ Context-dependent | âœ… Robust across contexts |
| **Training** | ğŸ”„ Requires fine-tuning | âœ… Zero additional training |
| **Interpretability** | â“ Limited insights | ğŸ” Clear mathematical basis |
| **Language Preservation** | âš ï¸ Often compromised | âœ… Maintained competence |
| **Computational Cost** | ğŸ’° High | ğŸ’¡ Minimal overhead |

---

## ğŸš€ Quick Start Guide

1. **ğŸ”§ Setup Environment**
   ```bash
   python3.8 -m venv venv && source venv/bin/activate
   pip install -r requirements.txt
   ```

2. **ğŸ® Run Full Pipeline**
   ```bash
   cd EigenShift
   python get_generations.py
   python toxic_words_extraction.py  
   python reconstruct.py
   python evaluation_after_intervention.py
   ```

3. **ğŸ“Š Analyze Results**
   - Check output files in `generations/` directory
   - Review evaluation metrics
   - Compare with baseline methods

---


## ğŸ“– Citation

If this work is helpful in your research, please cite:

```bibtex
soon
```

---
This project is licensed under [License Type] - see the LICENSE file for details.

---

*ğŸŒŸ Made with passion for AI Safety and Responsible AI Development*
